% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/unifiedFocalLoss.R
\name{defineBinaryLoss}
\alias{defineBinaryLoss}
\title{defineBinaryLoss}
\usage{
defineBinaryLoss(
  lambda = 0.5,
  gamma = 0.5,
  delta = 0.6,
  smooth = 1e-08,
  chnDim = TRUE,
  zeroStart = TRUE,
  useLogCosH = FALSE
)
}
\arguments{
\item{lambda}{Term used to control the relative weighting of the distribution- and region-based
losses. Default is 0.5, or equal weighting between the losses. If lambda = 1, only the distribution-
based loss is considered. If lambda = 0, only the region-based loss is considered. Values between 0.5
and 1 put more weight on the distribution-based loss while values between 0 and 0.5 put more
weight on the region-based loss.}

\item{gamma}{Parameter that controls increased weighting applied to difficult-to-predict pixels (for
distribution-based losses) or difficult-to-predict classes (region-based losses). Default is 0, or no focal
weighting is applied.}

\item{delta}{Parameter that controls the relative weightings of false positive and false negative errors for
each class. Different weightings can be provided for each class. The default is 0.6, which results in prioritizing
false positive errors relative to false negative errors.}

\item{smooth}{Smoothing factor to avoid divide-by-zero errors and provide numeric stability. Default is 1e-8.
Recommend using the default.}

\item{chnDim}{TRUE or FALSE. Whether the channel dimension is included in the target tensor:
(Batch, Channel, Height, Width) as opposed to (Batch, Channel, Height, Width). If the channel dimension
is included, this should be set to TRUE. If it is not, this should be set to FALSE. Default is TRUE.}

\item{zeroStart}{TRUE or FALSE. If class indices start at 0 as opposed to 1, this should be set to
TRUE. This is required  to implement one-hot encoding since R starts indexing at 1. Default is TRUE.}

\item{useLogCosH}{TRUE or FALSE. Whether or not to apply a logCosH transformation to the region-based
loss. Default is FALSE.}

\item{pred}{Tensor of predicted class logits. Should be of shape (mini-batch,
class, width, height) where the class dimension has a length equal to the number
of classes being differentiated. For a binary classification, output can be provided
as (mini-batch, class, width, height) or (mini-batch, width, height) if only the positive
case logit is returned.}

\item{target}{Tensor or predicted class indices from 0 to n-1 or 1 to n where n is the
number of classes. For a binary classification, only the positive case logit can be returned.
Shape can be (mini-batch, class, width, height) or (mini-batch, width, height)}
}
\value{
Loss metric for use in training process.
}
\description{
Define a loss for binary semantic segmentation using a modified unified focal loss framework as a subclass of torch::nn_module()
}
\details{
Use this loss as opposed to the multiclass version if only a logit for the positive case is returned.
Implementation of modified version of the unified focal dice loss after:

Yeung, M., Sala, E., Sch√∂nlieb, C.B. and Rundo, L., 2022. Unified focal loss:
Generalising dice and cross entropy-based losses to handle class imbalanced
medical image segmentation. Computerized Medical Imaging and Graphics, 95, p.102026.

This loss has three key hyperparameters that control its implementation. Lambda controls
the relative weight of the distribution- and region-based losses. Default is 0.5,
or equal weighting between the losses is applied. If lambda = 1, only the distribution-
based loss is considered. If lambda = 0, only the region-based loss is considered. Values between 0.5
and 1 put more weight on the distribution-based loss while values between 0 and 0.5 put more
weight on the region-based loss.

Gamma controls the application of focal loss and the application of
increased weight to difficult-to-predict pixels (for distribution-based losses). Focal correction is only applied to
distribution-based loss. Lower gamma values put increased weight on difficult samples or classes.
Using a value of 1 equates to not using a focal adjustment.

The delta term controls the relative weight of
false positive and false negative errors for each class. The default is 0.6 for each class, which results in
placing a higher weight on false positive as opposed to false negative errors relative to that class.
}
