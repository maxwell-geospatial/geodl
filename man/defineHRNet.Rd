% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/segModels.R
\name{defineHRNet}
\alias{defineHRNet}
\title{defineHRNet}
\usage{
defineHRNet(
  inChn = 3,
  nCls = 2,
  l1FMs = 32,
  l2FMs = 64,
  l3FMs = 128,
  l4FMs = 256,
  dcFMs = 256,
  dilChn = c(256, 256, 256, 256),
  dilRates = c(6, 12, 18),
  doRes = TRUE,
  actFunc = "lrelu",
  negative_slope = 0.01
)
}
\arguments{
\item{inChn}{Number of input predictor variables or channels. Default is 3.}

\item{nCls}{Number of output classes. Default is 2}

\item{l1FMs}{Number of feature maps to produce throughout the Level 1 layers. Default is 32.}

\item{l2FMs}{Number of feature maps to produce throughout the Level 2 layers. Default is 64.}

\item{l3FMs}{Number of feature maps to produce throughout the Level 3 layers. Default is 128.}

\item{l4FMs}{Number of feature maps to produce throughout the Level 4 layers. Default is 256.}

\item{dcFMs}{Number of feature maps to produce throughout the decoder blocks. Default is 256.}

\item{dilChn}{Vector of 4 values specifying the number of channels to produce at each dilation
rate within the ASPP module. Default is 256 for each dilation rate.}

\item{dilRates}{Vector of 3 values specifying the dilation rates used in the ASPP module.
Default is 6, 12, and 18.}

\item{doRes}{TRUE or FALSE. Whether or not to include residual connections in convolution
blocks of the encoder. Default is TRUE.}

\item{actFunc}{Defines activation function to use throughout the network. "relu" = rectified
linear unit (ReLU); "lrelu" = leaky ReLU; "swish" = swish. Default is "relu".}

\item{negative_slope}{If actFunc = "lrelu", specifies the negative slope term
to use. Default is 0.01.}
}
\value{
HRNet model instance as torch nn_module
}
\description{
Define a modified HRNet architecture inspired by:
}
\details{
Wang, J., Sun, K., Cheng, T., Jiang, B., Deng, C., Zhao, Y., Liu, D., Mu, Y.,
Tan, M., Wang, X. and Liu, W., 2020. Deep high-resolution representation learning
for visual recognition. IEEE transactions on pattern analysis and machine intelligence,
43(10), pp.3349-3364.

User can specify the number of input predictor variables or channels and the number of
output classes. The number of feature maps to generate throughout Levels 1 through 4
of the encoder and the decoder block. Residual connections can  be added around convolutional
layers within the blocks. The user can also choose between ReLU, leak ReLU, and swish.
}
\examples{
\donttest{
library(torch)
hrMod <- defineHRNet(inChn=8,
                     nCls=2,
                     l1FMs=64,
                     l2FMs=128,
                     l3FMs=128,
                     l4FMs=128,
                     dcChn=256,
                     dilRates=c(6,12,18),
                     dilChn=c(128,128,128),
                     actFunc = "relu",
                     negative_slope = 0.01)$to(device="cuda")

t1 <- torch_rand(12,8,256,256)$to(device="cuda")
p1 <- hrMod(t1)
}
}
