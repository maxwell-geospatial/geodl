% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/assessPnts.R
\name{assessPnts}
\alias{assessPnts}
\title{assessPnts}
\usage{
assessPnts(
  reference,
  predicted,
  multiclass = TRUE,
  mappings = levels(as.factor(reference)),
  decimals = 4
)
}
\arguments{
\item{reference}{Data frame column or vector of reference classes.}

\item{predicted}{Data frame column or vector of predicted classes.}

\item{multiclass}{TRUE or FALSE. If more than two classes are differentiated,
use TRUE. If only two classes are differentiated and there are positive and
background/negative classes, use FALSE. Default is TRUE.}

\item{mappings}{Vector of factor level names. These must be in the same order
as the factor levels so that they are correctly matched to the correct category.
If no mappings are provided, then the factor levels are used by default.}

\item{positive_case}{Factor level associated with the positive case for a
binary classification. Default is the second factor level. This argument is
not used for multiclass classification.}
}
\value{
List object containing the resulting metrics. For multiclass assessment,
the confusion matrix is provided in the $ConfusionMatrix object, the aggregated
metrics are provided in the $Metrics object, class user's accuracies are provided
in the $UsersAccs object, class producer's accuracies are provided in the
$ProducersAccs object, and the list of classes are provided in the $Classes object.
For a binary classification, the confusion matrix is provided in the
$ConfusionMatrix object, the metrics are provided in the $Metrics object,
the classes are provided in the $Classes object, and the positive class label is
provided in the $PositiveCase object.
}
\description{
Assess semantic segmentation model using point locations
}
\details{
This function generates a set of summary metrics when provided
reference and predicted classes. For a multiclass classification problem
a confusion matrix is produced with the columns representing the reference
data and the rows representing the predictions. The following metrics are
calculated: overall accuracy (OA), 95\% confidence interval for OA
(OAU and OAL), the Kappa statistic, map image classification
efficacy (MICE), average class user's accuracy (aUA), average class
producer's accuracy (aPA), average class F1-score, overall error (Error),
allocation disagreement (Allocation), quantity disagreement (Quantity),
exchange disagreement (Exchange), and shift disagreement (shift). For average
class user's accuracy, producer's accuracy, and F1-score, macro-averaging
is used where all classes are equally weighted. For a multiclass classification
all class user's and producer's accuracies are also returned.

For a binary classification problem, a confusion matrix is returned
along with the following metrics: overall accuracy (OA), overall accuracy
95\% confidence interval (OAU and OAL), the Kappa statistic (Kappa), map
image classification efficacy (MICE), precision (Precision), recall (Recall),
F1-score (F1), negative predictive value (NPV), specificity (Specificity),
overall error (Error), allocation disagreement (Allocation), quantity
disagreement (Quantity), exchange disagreement (Exchange), and shift
disagreement (shift).

Results are returned as a list object. This function makes use of the caret,
diffeR, and rfUtilities packages.
}
