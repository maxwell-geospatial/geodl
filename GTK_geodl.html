<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Getting Started with geodl</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="GTK_geodl_files/libs/clipboard/clipboard.min.js"></script>
<script src="GTK_geodl_files/libs/quarto-html/quarto.js"></script>
<script src="GTK_geodl_files/libs/quarto-html/popper.min.js"></script>
<script src="GTK_geodl_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="GTK_geodl_files/libs/quarto-html/anchor.min.js"></script>
<link href="GTK_geodl_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="GTK_geodl_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="GTK_geodl_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="GTK_geodl_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="GTK_geodl_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#what-are-the-system-requirements" id="toc-what-are-the-system-requirements" class="nav-link" data-scroll-target="#what-are-the-system-requirements">What are the system requirements?</a></li>
  <li><a href="#how-do-i-install-geodl" id="toc-how-do-i-install-geodl" class="nav-link" data-scroll-target="#how-do-i-install-geodl">How do I install geodl?</a></li>
  <li><a href="#overview-of-the-workflow" id="toc-overview-of-the-workflow" class="nav-link" data-scroll-target="#overview-of-the-workflow">Overview of the workflow</a></li>
  </ul></li>
  <li><a href="#data-prep" id="toc-data-prep" class="nav-link" data-scroll-target="#data-prep">Data prep</a></li>
  <li><a href="#unet-model" id="toc-unet-model" class="nav-link" data-scroll-target="#unet-model">UNet model</a>
  <ul class="collapse">
  <li><a href="#leaky-relu" id="toc-leaky-relu" class="nav-link" data-scroll-target="#leaky-relu">Leaky ReLU</a></li>
  <li><a href="#residual-blocks" id="toc-residual-blocks" class="nav-link" data-scroll-target="#residual-blocks">Residual blocks</a></li>
  <li><a href="#dilation" id="toc-dilation" class="nav-link" data-scroll-target="#dilation">Dilation</a></li>
  <li><a href="#atrous-spatial-pyramid-pooling-aspp" id="toc-atrous-spatial-pyramid-pooling-aspp" class="nav-link" data-scroll-target="#atrous-spatial-pyramid-pooling-aspp">Atrous Spatial Pyramid Pooling (ASPP)</a></li>
  <li><a href="#attention-gates" id="toc-attention-gates" class="nav-link" data-scroll-target="#attention-gates">Attention gates</a></li>
  <li><a href="#deep-supervision" id="toc-deep-supervision" class="nav-link" data-scroll-target="#deep-supervision">Deep supervision</a></li>
  <li><a href="#freezing-the-encoder-andor-backbone" id="toc-freezing-the-encoder-andor-backbone" class="nav-link" data-scroll-target="#freezing-the-encoder-andor-backbone">Freezing the encoder and/or backbone</a></li>
  </ul></li>
  <li><a href="#train-a-model" id="toc-train-a-model" class="nav-link" data-scroll-target="#train-a-model">Train a model</a>
  <ul class="collapse">
  <li><a href="#optimizers" id="toc-optimizers" class="nav-link" data-scroll-target="#optimizers">Optimizers</a></li>
  <li><a href="#loss-metrics" id="toc-loss-metrics" class="nav-link" data-scroll-target="#loss-metrics">Loss metrics</a></li>
  <li><a href="#assessment-metrics" id="toc-assessment-metrics" class="nav-link" data-scroll-target="#assessment-metrics">Assessment metrics</a></li>
  <li><a href="#deep-supervision-1" id="toc-deep-supervision-1" class="nav-link" data-scroll-target="#deep-supervision-1">Deep supervision</a></li>
  <li><a href="#selecting-a-learning-rate" id="toc-selecting-a-learning-rate" class="nav-link" data-scroll-target="#selecting-a-learning-rate">Selecting a learning rate</a></li>
  <li><a href="#incorporating-luz-callbacks" id="toc-incorporating-luz-callbacks" class="nav-link" data-scroll-target="#incorporating-luz-callbacks">Incorporating luz callbacks</a></li>
  </ul></li>
  <li><a href="#model-assessment-and-inference" id="toc-model-assessment-and-inference" class="nav-link" data-scroll-target="#model-assessment-and-inference">Model assessment and inference</a>
  <ul class="collapse">
  <li><a href="#assessment" id="toc-assessment" class="nav-link" data-scroll-target="#assessment">Assessment</a></li>
  <li><a href="#inference" id="toc-inference" class="nav-link" data-scroll-target="#inference">Inference</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Getting Started with geodl</h1>
<p class="subtitle lead">Geospatial deep learning with R, torch, and terra</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="quarto-figure quarto-figure-right" style="float: right; max-px;">
<figure class="figure">
<p><img src="images/geodlHex-01.png" class="img-fluid figure-img" width="100"></p>
</figure>
</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Welcome to <strong>geodl</strong>. The goal of this package is to support geospatial deep learning in the R data science language and computational environment. Specifically, it focuses on <strong>semantic segmentation</strong>, or pixel-level, classification tasks and makes use of <a href="https://torch.mlverse.org/"><strong>torch</strong></a>, which is an R package that provides graphics processing unit (GPU)-accelerated tensor manipulation to support deep learning. One convenient characteristic of torch is that it does not make use of Python. It is not simply a wrapper around PyTorch. Instead, it interfaces with libtorch and C++ code, which simplifies the installation and computational environment. Raster geospatial data are handled with the <a href="https://cran.r-project.org/web/packages/terra/index.html"><strong>terra</strong></a> package, and the model training can be implemented with <a href="https://cran.r-project.org/web/packages/luz/index.html"><strong>luz</strong></a>. The <a href="https://cran.r-project.org/web/packages/torchvision/index.html"><strong>torchvision</strong></a> package is also used to handle image-like data arrays or tensors and to perform data augmentations.</p>
<p><a href="https://pytorch.org/"><strong>PyTorch</strong></a>, the Python library that interfaces with libtorch, has a large user base and ecosystem of additional packages that expand and/or simplify its use. Unfortunately, torch in R does not have this same level of development to date. As a result, geodl was developed to make it easier to use torch for geospatial semantic segmentation deep learning. Specifically, geodl provides functions and utilities to:</p>
<ol type="1">
<li>Generate raster-based <strong>masks</strong> from reference vector geospatial data</li>
<li>Generate <strong>image chips</strong> from input raster-based predictor variables and raster masks</li>
<li>List image/mask chip names and paths into a data frame</li>
<li>View image chip subsets</li>
<li>Calculate statistics from image chips and masks</li>
<li>Define a <strong>DataSet</strong> for use in a <strong>DataLoader</strong> and apply random augmentations as needed</li>
<li>View data generated by a DataSet/DataLoader</li>
<li>Define a <strong>UNet</strong>-like architecture for semantic segmentation</li>
<li>Train a semantic segmentation model using custom <strong>loss</strong> and <strong>assessment metrics</strong></li>
<li>Use a trained model to predict to new spatial data extents without the need to generate chips</li>
<li>Assess models using remote sensing metrics and standards</li>
</ol>
<p>geodl is still experimental. Please let us know if you have any issues, suggestions, or would like to contribute. Our goal is to further refine this package and eventually release it to CRAN.</p>
<section id="what-are-the-system-requirements" class="level3">
<h3 class="anchored" data-anchor-id="what-are-the-system-requirements">What are the system requirements?</h3>
<p>You should use R version &gt;=4.1. The following packages are also required: dplyr, terra, diffeR, caret, rfUtilities, and MultiscaleDTM. The most recent version of these packages should be used.</p>
<p>If you plan to train models, it is recommended to have a computer with a <a href="https://developer.nvidia.com/cuda-gpus"><strong>CUDA</strong>-enabled GPU</a>. You will also need to set up CUDA and the <strong>cuDNN</strong> library on your computer. <a href="https://cran.r-project.org/web/packages/torch/vignettes/installation.html">This</a> vignette provides information on how to set up CUDA on your machine for GPU-based computation. Note that torch in R cannot yet make use of multiple GPUs.</p>
</section>
<section id="how-do-i-install-geodl" class="level3">
<h3 class="anchored" data-anchor-id="how-do-i-install-geodl">How do I install geodl?</h3>
<p>geodl is not yet available on CRAN. However, it can be installed from <a href="https://github.com/maxwell-geospatial/geodl">GitHub</a> using the <a href="https://cran.r-project.org/web/packages/remotes/index.html">remotes</a> package.</p>
</section>
<section id="overview-of-the-workflow" class="level3">
<h3 class="anchored" data-anchor-id="overview-of-the-workflow">Overview of the workflow</h3>
<p>Figure 1 provides an overview of the geodl workflow. Functions prefixed with torch:: are from the torch package, and those prefixed with luz:: are form the luz package. Functions with a green check mark indicate those that are used as checks during the process or to assess trained models. Purple diamonds indicate outputs or results: the trained model and predictions to new raster data. The remainder of this document describes the components/functions of the workflow and package.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Fig1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure 1. geodl workflow.</figcaption><p></p>
</figure>
</div>
</section>
</section>
<section id="data-prep" class="level2">
<h2 class="anchored" data-anchor-id="data-prep">Data prep</h2>
<p>Functions provided by geodl support the production of image chips and associated masks to train semantic segmentation models. The generated chips can then be listed to a data frame object and referenced within a DataLoader. The key preparation functions are briefly described here. Our goal in this document is to provide an introduction to these functions. For a more detailed discussion of the functions and their associated parameters, please see the package documentation.</p>
<ul>
<li><strong>makeMasks()</strong>: generate categorical raster masks from vector-based geospatial data. This function can also crop images to a define extent to align them with the generated masks. Images and associated masks must have the same spatial extent, origin, coordinate reference system, and cell size if our functions/workflow will be used to generate chips from them. The user must select a column from the vector data that provides a unique numeric code for each class. You should start class codes at 0 and not skip any values (i.e., class codes should be 0 to <em>n</em>-1). For binary classification problems, the background class should be coded to 0 and the presence or positive class should be coded to 1.</li>
<li><strong>makeChips()/makeChipsMultiClass()</strong>: break larger image extents and associated raster masks into smaller image chips of a defined size or number of rows and columns of pixels. The makeChips() function should be used for binary classification where the positive class is assigned a value of 1 and the background class is assigned a value of 0. When more than two classes are differentiated using unique numeric codes, the makeChipsMultiClass() function should be used. If you have sparse labels (i.e., not all pixels are labelled to a class even though they belong to a specific class), these pixels should be assigned a numeric code that can then be referenced in the loss and/or assessment metrics to be ignored. For a binary classification problem, all chips can be generated (“All”), just those containing at least one pixel mapped to the positive case (“Positive”), or both background-only and positive case chips written to separate folders (“Divided”). If a chip is incomplete or has NA/NoDATA values, generally resulting from a non-rectangular raster extent, that chip will not be written to disk; only complete chips are produced. Chips are written to a TIFF file format. The masks will be a single band raster where each cell is assigned to a class index. Predictor variables are written as multiband raster grids.</li>
<li><strong>makeChipsDF()</strong>: create a data frame listing all image chip names and the path to the image and mask chips. If the “Divided” method is used, a column is added to note whether the chip contains positive class cells or just background cells.</li>
<li><strong>describeChips()</strong>: obtain summary statistics from image chips and masks. These results can be used to specify <strong>normalization</strong> or <strong>rescaling</strong> parameters within defineSegDataSet() or defineSegDataSetDS() or to determine class weights to deal with class imbalance. This function is also useful as a check to make sure the range of predictor variable values and class codes are as expected.</li>
<li><strong>viewChips()</strong>: plot a random subset of image chips and associated masks. The primary purpose of this function is to offer a visual check of image chips.</li>
<li><strong>defineSegDataSet()</strong>/<strong>defineSegDataSetDS()</strong>: define a DataSet subclass that references the chips data frame. This function also provides options to apply <strong>random data augmentations</strong> to potentially combat <strong>overfitting</strong>. If a chip is flipped, the mask will also be flipped for alignment. The user can also specify normalization or rescaling parameters. It is generally recommended that the predictor variables be scaled such that the data range is 0 to 1. For example, a rescaling factor can be used to convert 8-bit image channels (0 to 255) to this scale. If you are implementing deep supervision, described below, you should use defineSegDataSetDS().</li>
<li><strong>viewBatch()</strong>: provides a visual check of a batch of data generated using defineSegDataSet()/defineSegDataSetDS() and a DataLoader.</li>
<li><strong>describeBatch()</strong>: provides a check of a data batch. This can be used to confirm that the data are scaled as expected, in the correct configuration (Batch, Channels, Height, Width), and have the correct data types. This can also be used to make sure the associated masks have the expected numeric codes, shape, and data type.</li>
<li><strong>makeTerrainDerivatives()</strong>: creates a three-band raster stack from an input <strong>digital terrain model</strong> (<strong>DTM</strong>) of bare earth surface elevations. The first band is a <strong>topographic position index</strong> (<strong>TPI</strong>) calculated using a moving window with a 50 m circular radius.The second band is the square root of slope calculated in degrees. The third band is a TPI calculated using an annulus moving window with an inner radius of 2 meters and outer radius of 5 meters. The TPI values are clamped to a range of -10 to 10 then linearly rescaled from 0 and 1. The square root of slope is clamped to a range of 0 to 10 then linearly rescaled from 0 to 1. Values are provided in floating point. This means of repenting digital terrain data is discussed in the following publication, which is available in open-access:</li>
</ul>
<p>Maxwell, A.E., W.E. Odom, C.M. Shobe, D.H. Doctor, M.S. Bester, and T. Ore, 2023. Exploring the influence of input feature space on CNN-based geomorphic feature extraction from digital terrain data, <em>Earth and Space Science</em>, 10: e2023EA002845. https://doi.org/10.1029/2023EA002845].</p>
</section>
<section id="unet-model" class="level2">
<h2 class="anchored" data-anchor-id="unet-model">UNet model</h2>
<p>The <strong>defineUNet()</strong> function provides a flexible means to generate a UNet-like architecture for semantic segmentation tasks, which is conceptualized in Figure 1. Here are the primary characteristics of the implementation:</p>
<ul>
<li>accepts a variable number of input predictor variables or channels, defined by the <em>nChn</em> parameter, and output classes, defined by the <em>nCls</em> parameter.</li>
<li>predicts a <strong>logit</strong> for each class in the case of a <strong>multiclass classification</strong> (logits are not passed through a softmax activation to obtain probabilities).</li>
<li>for a <strong>binary classification</strong>, can predict a single logit for the positive case or 2 logits corresponding to the positive and negative case.</li>
<li>contains 4 <strong>encoder</strong> blocks, a <strong>bottleneck</strong> block, and 4 <strong>decoder</strong> blocks</li>
<li>allows the users to specify the number of output feature maps from each encoder block (<em>encoderChn</em> parameter), the bottleneck block (<em>botChn</em> parameter), and each decoder block (<em>decoderChn</em> parameter). The default number of output encoder feature maps per encoder block are 16, 32, 64, and 128; the default number of output feature maps for the bottleneck is 256; the default number of output feature maps for the decoder blocks are 128, 64, 32, and 16.</li>
<li>by default, uses the <strong>rectified linear unit</strong> (<strong>ReLU</strong>) activation function throughout the architecture to incorporate non-linearity and uses <strong>batch normalization</strong> to combat gradient issues and aid in convergence.</li>
<li>Uses <strong>2D transpose convolution</strong> to upscale the feature maps in the decoder blocks.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Fig2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure 2. UNet architecture implemented in geodl.</figcaption><p></p>
</figure>
</div>
<p>We have also implemented some options for further customizing the architecture.</p>
<ul>
<li>replace rectified linear unit (ReLU) activation with <strong>leaky ReLU</strong> and define the negative slope term.</li>
<li>replace the double convolution layers in the encoder, bottleneck, and decoder with <strong>residual double convolution layers</strong>.</li>
<li>replace the double convolution layers in the encoder and decoder with <strong>dilated convolution</strong> with multiple dilation rates with or without residual connections.</li>
<li>replace the bottleneck layer with an <strong>atrous spatial pyramid pooling</strong>-like (<strong>ASPP</strong>) module.</li>
<li>add <strong>attention gating</strong> mechanisms.</li>
<li>use <strong>deep supervision</strong>.</li>
<li>freeze the encoder and/or bottleneck so that only the decoder is trained.</li>
</ul>
<section id="leaky-relu" class="level3">
<h3 class="anchored" data-anchor-id="leaky-relu">Leaky ReLU</h3>
<p>The traditional ReLU activation function adds non-linearity to the model by converting all negative activations to 0 and maintaining the positive activations. Leaky ReLU augments this by allowing for maintaining the negative activations but with a reduced magnitude by applying a negative slope term. Leaky ReLU may help alleviate the vanishing gradient problem. If you want to use leaky ReLU in place of ReLU, set the <em>useLeaky</em> parameter to TRUE. You can also specify the <em>negative_slope</em> parameter, which is 0.01 by default if leaky ReLU is used.</p>
</section>
<section id="residual-blocks" class="level3">
<h3 class="anchored" data-anchor-id="residual-blocks">Residual blocks</h3>
<p>The traditional double-convolution layers used in UNet consist of passing the input feature maps through a 2D convolution with a kernel size of (3,3), a stride of 1, and a padding of 1 to produce a set of feature maps equal to the number of input feature maps. These results are then passed through a second 2D convolution layer with a kernel size of (3,3), a stride of 1, and a padding of 1 to generated the user-defined number of output feature maps for that stage in the architecture. A <strong>residual connection</strong> or <strong>residual block</strong> augments this architecture by adding the input feature maps to the output from the second 2D convolution (Figure 2). Note that this is an actual addition of the feature maps, not a concatenation. The goal is to potentially reduce the vanishing gradient issue by maintaining this original signal in the the output. To use residual connections, set the <em>useRes</em> parameter to TRUE.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Fig6.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure 3. Residual block.</figcaption><p></p>
</figure>
</div>
</section>
<section id="dilation" class="level3">
<h3 class="anchored" data-anchor-id="dilation">Dilation</h3>
<p><strong>Dilated</strong> or <strong>atrous convolution</strong> consists of adding zeros into the moving window or kernel so that convolution can be performed using cells that are not direct neighbors. The goal is to increase the receptive field to capture spatial patterns at varying scales. If the <em>useDilation</em> parameter is set to TRUE, the traditional double convolution layers in the encoder will be replaced by an augmented block that incorporates dilation at 4 different rates. Padding is used to maintain the size of the feature maps in the spatial dimensions so that results from each dilation rate can be concatenated. Once the feature maps from all 4 dilated convolution blocks are concatenated, they are passed through a second 2D convolution without dilation. This architecture is conceptualized in Figure 3.</p>
<p>If you want to use dilation, set the <em>useDilation</em> parameter to TRUE. The <em>dilRates</em> parameter specifies the dilation rate to use in each of the 4 dilated convolution components, which are 1, 3, 5, and 7 by default. The <em>perDilChn</em> parameter specifies how many feature maps to produce within each dilated convolution block. Since there are 4 dilated convolutions layers that are then concatenated before being passed through the final 2D convolution layer within the block, the total number of feature maps produced will be 4 times <em>perDilChn</em>. The final number of feature maps from each block following the second 2D convolution is controlled by the <em>encoderChn</em> parameter.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Fig7.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure 4. Dilated convolution blocks.</figcaption><p></p>
</figure>
</div>
</section>
<section id="atrous-spatial-pyramid-pooling-aspp" class="level3">
<h3 class="anchored" data-anchor-id="atrous-spatial-pyramid-pooling-aspp">Atrous Spatial Pyramid Pooling (ASPP)</h3>
<p><strong>Atrous spatial pyramid pooling</strong> (<strong>ASPP</strong>) is similar to the concept of dilated convolution. The goal is to capture spatial context at varying scales to increase the size of the receptive field. This technique is applied within the <a href="https://arxiv.org/abs/1802.02611">DeepLabv3+ architecture</a>. We implement a modified version of this module here (Figure 4). If the <em>useASPP</em> argument is set to TRUE, the bottleneck layer is replaced with the modified ASPP module. This module accepts the feature maps from the 4th encoder block without applying max pooling. Dilated convolution is then performed using dilation rates of 1, 2, 4, 8, and 16 by default. These rates can be changed using the <em>asppDilRates</em> parameter. The <em>asppChnOut</em> parameter defines the number of features maps generated by each of the dilated convolution layers and is 64 by default. As a result, the number of feature maps produced will be 5 times <em>asppChnOut</em>. The results are then concatenated and passed through a 2D convolution layer with a kernel size of (1,1), a stride of 1, and a padding of 0. The number of output feature maps from this final layer is controlled by the <em>botChn</em> parameter, same as the default bottleneck layer that does not use the ASPP module.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Fig4-01.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure 5. Atrous Spatial Pyramid Pooling (ASPP)-like module implemented in geodl.</figcaption><p></p>
</figure>
</div>
</section>
<section id="attention-gates" class="level3">
<h3 class="anchored" data-anchor-id="attention-gates">Attention gates</h3>
<p><a href="https://arxiv.org/abs/1804.03999"><strong>Attention gates</strong></a> provide a mechanism to allow for forcing the model to focus on key features or regions within the image. The idea is to use the results from the subsequent layer in the network, where a deeper set of features have been extracted, to add focus or attention to the feature maps from the prior layer that are then concatenated with the upscaled feature maps from the following block and fed to the decoder block. This process is implemented as follows and is conceptualized in Figure 5:</p>
<ol type="1">
<li>The feature maps from the next layer in the sequence (for example, the feature maps produced by encoder block 3 when the attention gate will be applied to the feature maps from encoder block 2) are passed through a 2D convolution layer with a kernel size of (1,1), a stride of 1, and a padding of 0. The number of channels are changed to match those from the prior block. A batch normalization is then applied.</li>
<li>The feature maps from the current layer are passed through a 2D convolution layer with a kernel size of (1,1), a stride of 2, and a padding of 0. The number of output feature maps are equal to the number of input feature maps. Since a stride of 2 is used, this will decrease the spatial resolution by half such that the size is the same as those from the next block. Batch normalization is then applied.</li>
<li>The gating signal from Step 1 and the feature maps from Step 2 are then added together and passed through a ReLU activation.</li>
<li>The results from Step 3 are passed through a 2D convolution with a kernel size of (1,1), a stride of 1, and a padding 0 to produce a single output feature map. This feature map is then passed through a batch normalization layer followed by a sigmoid activation. This results is values between 0 and 1 where values near 1 highlight areas that should be focused on or areas to which attention should be applied.</li>
<li>In order to return the original spatial resolution of the input feature maps, upsampling is then applied using <strong>bilinear interpolation</strong>.</li>
<li>The original feature maps from the encoder block of interest are multiplied by the result from Step 5.</li>
<li>The results from Step 6 are concatenated with the upsampled feature maps from the next block, which are first upscaled using 2D transpose convolution, to be fed into the associated decoder block as normal.</li>
</ol>
<p>If you would like to implement attention gates, set the <em>useAttn</em> argument to TRUE.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Fig5.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure 6. Attention gate module implemented in geodl.</figcaption><p></p>
</figure>
</div>
</section>
<section id="deep-supervision" class="level3">
<h3 class="anchored" data-anchor-id="deep-supervision">Deep supervision</h3>
<p><strong>Deep supervision</strong> consists of generating predictions using the output from each decoder block at the spatial resolution of that block as opposed to only making a prediction at the end of the final decoder block (Figure 5). Losses are then calculated for each of these results. The goal is to further guide the generation of feature maps in the intermediate layers. If you want to use deep supervision, set the <em>useDS</em> parameter to TRUE. We will discuss deep supervision in more detail below in the context of the training process.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Fig3.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure 7. Deep supervision implementation.</figcaption><p></p>
</figure>
</div>
</section>
<section id="freezing-the-encoder-andor-backbone" class="level3">
<h3 class="anchored" data-anchor-id="freezing-the-encoder-andor-backbone">Freezing the encoder and/or backbone</h3>
<p>If a model has already been trained, it is possible to use the learned parameters to initiate a new model. This is one way to implement <strong>transfer learning</strong>. You should only freeze the encoder and/or bottleneck if you are initiating a model using learned parameters as opposed to random parameters. Also, if you freeze the backbone, you should also freeze the encoder. Freezing can be implemented by setting the <em>freezeEncoder</em> and/or <em>freezeBackone</em> parameter(s) to TRUE. When set to TRUE, this will not allow the parameters in the encoder and/or backbone to be updated during the training process.</p>
</section>
</section>
<section id="train-a-model" class="level2">
<h2 class="anchored" data-anchor-id="train-a-model">Train a model</h2>
<p>Once data have been prepared and a model has been configured, it can be trained. We highly recommend using the <a href="https://cran.r-project.org/web/packages/luz/index.html">luz</a> package to train models as opposed to defining your own training loop since the training loop is prone to errors. There are a few key considerations when setting up your training process. Our provided examples demonstrate the training process. Here, we will point out key considerations and make some general recommendations.</p>
<ul>
<li>Data should be partitioned into separate and non-overlapping <strong>training</strong>, <strong>validation</strong>, and <strong>testing</strong> sets. The training data are used in the learning process to update the model parameters based on prediction error and the calculated loss. The <strong>validation</strong> data are generally predicted at the end of each training epoch to assess how well the model is performing when predicting to new data. The use of validation data are important to assess for <strong>overfitting</strong> and model <strong>generalization</strong>. The testing data are used to assess the final model and are generally not used during the training loop.</li>
<li>You need to set a <strong>batch size</strong> in the DataLoader. This will partially be dictated by your hardware, especially the amount of VRAM available in your graphics card. If you run out of memory during the training process, you may need to decrease the batch size. Again, you will need access to a GPU to train semantic segmentation models.</li>
<li>You will need to specify a number of epochs over which to train the model. An <strong>epoch</strong> is one complete iteration over the training data.</li>
<li>You will need to select an <strong>optimizer</strong>. Optimizers also have <strong>hyperparameters</strong>, such as the <strong>learning rate</strong>. We suggest using the <strong>lr_finder()</strong> function from luz to determine an appropriate learning rate. This is demonstrated in our examples.</li>
<li>You will need to select a loss metric. This will partially be dictated by the type of problem (i.e., binary vs.&nbsp;multiclass) and issues of data imbalance between classes.</li>
<li>You can choose to monitor different assessment metrics. Appropriate assessment metrics will vary by problem type.</li>
<li>You may choose to augment or further control the learning process using <strong>callbacks</strong>. Several useful callbacks have been implemented with luz and will be discussed below.</li>
</ul>
<section id="optimizers" class="level3">
<h3 class="anchored" data-anchor-id="optimizers">Optimizers</h3>
<p>torch provides several <strong>optimizers</strong> including:</p>
<ul>
<li>Adadelta: <strong>optim_adadelta()</strong></li>
<li>Adagrad: <strong>optim_adagrad()</strong></li>
<li>Adam: <strong>optim_adam()</strong></li>
<li>AdamW: <strong>optim_adamw()</strong></li>
<li>LBFGS: <strong>optim_lbfgs()</strong></li>
<li>RMSprop: <strong>optim_rmsprop()</strong></li>
<li>Rprop: <strong>optim_rprop()</strong></li>
<li>Stochastic Gradient Descent (SGD): <strong>optim_sgd()</strong></li>
</ul>
<p>There is no preferred optimizer for all use cases. We generally prefer to use <strong>optim_adamw()</strong>; however, others may disagree. For more information about optimizers, please see the <a href="https://cran.r-project.org/web/packages/torch/torch.pdf">torch</a> documentation.</p>
</section>
<section id="loss-metrics" class="level3">
<h3 class="anchored" data-anchor-id="loss-metrics">Loss metrics</h3>
<p>The choice of <strong>loss</strong> metric is very important, as this measure serves as the sole guide to updating the model parameters during the learning process using <strong>backpropogation</strong> of errors and the optimizer. torch provides several loss functions natively using both function- and class-based implementations. Class-based implementations subclass <strong>nn_module()</strong> from torch.</p>
<ul>
<li><strong>Binary cross-entropy</strong> (<strong>nnf_binary_cross_entropy_with_logits()</strong>/<strong>nn_bce_with_logits_loss()</strong>): This loss is for binary classification where only the logit for the positive case is returned. There is also a version that accepts probabilities as opposed to logits (<strong>nn_bce_loss()</strong>); however, since our UNet-like architecture returns logits as opposed to probabilities, you should use the “with_logits” version. To use <strong>nn_bce_loss()</strong>, you would need to first pass the logits through a sigmoid activation.</li>
<li><strong>nnf_cross_entropy()/nn_cross_entropy_loss()</strong>: This is for multiclass classification problems or binary classification problems where both the positive and background logits are returned. This loss expects logits as opposed to probabilities (i.e., the logits have not been passed through a softmax activation). As a result, it is compatible with our UNet-like architecture. When using cross-entropy loss, you can also specify class weights, which can be useful for combating class imbalance. In the case of sparse labels, you can assign the “missing” class to a weight of zero so that these pixels do not impact of the parameter updates or learning process.</li>
</ul>
<p>Additional loss metrics have been proposed that can be especially useful when class proportions in the training set are imbalanced and/or when you want to have more control over the relative weightings of <strong>false positive</strong> and <strong>false negative</strong> errors. Unfortunately, these additional losses have not been implemented in torch. As a result, we have provided utilities for using and calculating some additional losses as part of geodl. These metrics are provided by the <strong>defineDiceFamilyLoss()</strong> function, which subclasses nn_module(). A function-based version is also provided as <strong>dice_loss_family()</strong>, which is called internally by the class-based version.</p>
<p>For binary classification problems, the <strong>Dice</strong>, <strong>Focal Dice</strong>, <strong>Tversky</strong>, or <strong>Focal Tversky</strong> loss is often used. Dice loss is simply 1 - Dice or 1 - F1-score. Dice and F1-score are equivalent. Tversky allows for specifying the relative weights of false positive and false negative errors using <strong>alpha</strong> and <strong>beta</strong> terms, respectively. The focal version of these losses adds a <strong>gamma</strong> term, which can be used to control the relative weighting of difficult to classify examples. Values larger than 1 will put more weight on difficult to classify samples. It is also possible to combine one of these losses with binary cross-entropy, generally termed <strong>combo loss</strong>. In order to have control over the relative contribution of the two losses, a weighting term is generally applied.</p>
<p>These metrics can be extended for multiclass classification problems, or a binary classification problem where both the positive and negative logit are returned. When doing so, it is important to consider how the class-level metrics are aggregated. <strong>Micro-averaging</strong> consists of pooling all <strong>true positive</strong>, <strong>true negative</strong>, false positive, and false negative predictions to calculate the metric. Classes with more samples will have a higher weight in the calculation. In contrast, <strong>macro-averaging</strong> consists of calculating the metric for each class separately then averaging the results. Using this method, classes will take on equal weight in the calculation. Alternatively, you can also incorporate class weights to obtain a class-weighted, macro-averaged metric. These multiclass losses can also be combined with cross-entropy loss as a combo loss. A weighting factor can be specified to control the relative contribution of each loss. Class weights can also be used for cross-entropy and the macro-averaged losses.</p>
<p>It should also be noted that Dice- and Tversky-based losses make use of the predicted class probabilities, obtained by applying a sigmoid or softmax activation, as opposed to the “hard” classification, as is the case when Dice, or the equivalent F1-score, is used as an assessment metric as opposed to a loss metric.</p>
<p>Below we have provided the input parameters for our <strong>defineDiceFamilyLoss()</strong> function.</p>
<ul>
<li><em>smooth</em>: smooth factor to aid in stability and to prevent divide-by-zero errors. Default is 1.</li>
<li><em>mode</em>: either “multiclass” or “binary”. Default is “multiclass”. If “multiclass”, the prediction should be provided as (Batch, Channel, Height, Width), where the channel dimension provides the predicted logit for each class, and the target should be (Batch, Channel, Height, Width), where the channel dimension provides the index for the correct class. Script assumes class indices start at 0 as opposed to 1. If “binary”, the prediction should be provided as (Batch, Channel, Height, Width), where the channel dimension provides the predicted logit for the positive class, and the target should be (Batch, Channel, Height, Width), where the channel dimension provides the index for the correct class (0 = negative, 1 = Positive). If the target does not included the channel dimension (i.e.&nbsp;(Batch, Height, Width)), the chnDim argument should be set to FALSE, which will force the script to add the channel dimension. It is best to provide targets in a torch_long data type. If not, the script will convert the targets to long. Predictions should be provided as logits, and a softmax or sigmoid activation should not be applied. The data type for the predictions should be torch_float32.</li>
<li><em>alpha</em>: alpha parameter for false positives in Tversky calculation. This is ignored if Dice is calculated. The default is 0.5.</li>
<li><em>beta</em>: beta parameter for false negatives in Tversky calculation. This is ignored if Dice is calculated. The default is 0.5</li>
<li><em>gamma</em>: gamma parameter if Focal Tversky or Focal Dice is calculated. Ignored if focal loss is not used. Default is 1.</li>
<li><em>average</em>: either “micro” or “macro”. Class averaging method applied for multiclass classification. If “micro”, classes are weighted relative to their abundance in the target data. If “macro”, classes are equally weighted in the calculation. Default is “micro”.</li>
<li><em>tversky</em>: TRUE or FALSE. Whether to calculate Tversky as opposed to Dice loss. If TRUE, Tversky is calculated. If FALSE, Dice is calculated. Default is FALSE.</li>
<li><em>focal</em>: TRUE or FALSE. Whether to calculate a Focal Dice or Focal Tversky loss. If FALSE, the gamma parameter is ignored. Default is FALSE.</li>
<li><em>combo</em>: TRUE or FALSE. Whether to calculate a combo loss using Dice/Tversky + binary entropy/cross entropy. If TRUE, a combo loss is calculated. If FALSE, a combo loss is not calculated. Default is FALSE.</li>
<li><em>useWghts</em>: TRUE or FALSE. Default is FALSE. If TRUE, class weights will be applied in the calculation of cross entropy loss and macro-average Dice or Tversky loss. This setting does not impact micro-averaged Dice or Tversky loss. If TRUE, the wght argument must be specified.</li>
<li><em>wght</em>: TRUE or FALSE. Default is FALSE. Must be defined if useWhgts is TRUE. A vector of class weights must be provided that has the same length as the number of classes. The vector is converted to a torch tensor within the script.</li>
<li><em>ceWght</em>: if combo is TRUE, defines relative weighting of binary cross entropy/cross entropy in the loss as (Dice/Tversky) + ceWght*(binary cross entropy/cross entropy). Ignored if combo is FALSE. Default is 1, or equal weighting in the calculation between the two losses.</li>
<li><em>chnDim</em>: TRUE or FALSE. Default is TRUE. If TRUE, assumes the target tensor includes the channel dimension: (Batch, Channel, Height, Width). If FALSE, assumes the target tensor does not include the channel dimension: (Channel, Height, Width). The script is written such that it expects the channel dimension. So, if FALSE, the script will add the channel dimension as needed.</li>
<li><em>mskLong</em>: TRUE or FALSE. Default is TRUE. Data type of target or mask. If the provided target has a data type other than torch_long, this parameter should be set to FALSE. This will cause the script to convert the target to the tensor_long data type as required for the calculations.</li>
</ul>
<p>If deep supervision is implemented, you must use the <strong>defineDiceFamilyLossDS()</strong> version of the function. This version expects 4 resulting predictions at different spatial scales as opposed to 1. The <em>dsWghts</em> parameter controls the relative weights of each loss in the final, combined loss. The argument for this parameter should be a vector of 4 weights that specify the relative contribution of each loss in the combined deep supervision loss. The 1st weight is applied to the result at the original spatial resolution, the 2nd at the 2nd spatial resolution (H/2, W/2), the 3rd at the 3rd spatial resolution (H/4, W/4), and the 4th at the 4th spatial resolution (H/8, W/8). The default is (1, .33, .33, .33). The final loss is the weighted average of the 4 losses.</p>
</section>
<section id="assessment-metrics" class="level3">
<h3 class="anchored" data-anchor-id="assessment-metrics">Assessment metrics</h3>
<p>Several assessment metrics are provided by the luz package. <strong>luz_metric_accuracy()</strong> can be used to obtain overall accuracy for a multiclass classification. For a binary classification where only the positive case logit is returned, you should use <strong>luz_metric_binary_accuracy_with_logits()</strong> as opposed to <strong>luz_binary_accuracy()</strong> since our UNet-like architecture returns logits as opposed to class probabilities. luz also provides the <strong>luz_metric()</strong> function for defining new or custom metrics. The geodl package made use of this function to create implementations of <strong>recall</strong>, <strong>precision</strong>, and <strong>F1-score</strong>.</p>
<p><strong>luz_metric_recall()</strong>, <strong>luz_metric_precision()</strong>, and <strong>luz_metric_f1_score()</strong> can be used for both binary and multiclass assessment. If only the positive case logit is returned for a binary classification, you should set the <em>mode</em> parameter to “binary”. When two or more classes are returned, you should set the <em>mode</em> argument to “multiclass”. For multiclass assessment, the <em>average</em> method controls how class-level metrics are combined. “macro” implements <strong>macro-averaging</strong>, in which the metric is calculated separately for each class and then averaged. Each class has equal weight in the resulting metric. In contrast, “micro” implements <strong>micro-averaging</strong> where results are collapsed to true positive, true negative, false positive, and false negative predictions to calculate a single metric. Using this mode, classes with more samples will take on a higher weight in the resulting calculation. It is important to note that recall, precision, F1-score, and overall accuracy are equivalent when micro-averaging is used. So, there is not need to calculate all of these metrics using micro-averaging. We recommend just calculating overall accuracy.</p>
</section>
<section id="deep-supervision-1" class="level3">
<h3 class="anchored" data-anchor-id="deep-supervision-1">Deep supervision</h3>
<p>As noted above, deep supervision consists of generating predictions using the output from each decoder block at the spatial resolution of that block as opposed to only making a prediction at the end of the final decoder block. Losses are then calculated for each of these results. The goal is to further guide the generation of feature maps in the intermediate layers.</p>
<p>In order implement deep supervision, you will need to do the following:</p>
<ol type="1">
<li>Use the <strong>defineSegDatasetDS()</strong> function as opposed to the define <strong>defineSegDataset()</strong> function to build your DataSet. The key difference is that the DS version returns the input predictor variables at the original spatial resolution and the masks at 4 spatial resolutions as opposed to just returning the original masks. The reduced spatial resolution masks are generated using the <strong>aggregate()</strong> function from terra using the “modal” method where the majority or most commonly occurring class from the cells being aggregated into a larger pixel is returned.</li>
<li>Set the <em>useDS</em> argument to TRUE for this parameter in the <strong>defineUNet()</strong> function.</li>
<li>Use deep supervision versions of the losses. As noted above, <strong>defineDiceFamilyLossDS()</strong> allows for using Dice, Focal Dice, Tversky, Focal Tversky, or a combo loss configured for deep supervision. The <em>dsWghts</em> parameter controls the relative weights of each loss in the final, combined loss. The argument for this parameter should be a vector of 4 weights that specify the relative contribution of each loss in the combined deep supervision loss. The 1st weight is applied to the result at the original spatial resolution, the 2nd at the 2nd spatial resolution, the 3rd at the 3rd spatial resolution, and the 4th at the 4th spatial resolution. The default is (1, .33, .33, .33). The final loss is the weighted average of the 4 losses. We have also provided augmented version of binary cross-entropy and cross-entropy losses (<strong>binary_loss_DS()</strong>/<strong>defineBceCeLoss()</strong>) for use with deep supervision that are based on the original torch implementations.</li>
</ol>
<p>Additionally, you will need to set the <em>usedDS</em> argument to TRUE in several other functions including <strong>viewBatch()</strong>, <strong>describeBatch()</strong>, <strong>viewBatchPreds()</strong>, and <strong>predictSpatial()</strong>. This argument must also be set to TRUE for assessment metrics: <strong>luz_metric_recall()</strong>, <strong>luz_metric_precision()</strong>, and <strong>luz_metric_f1_score()</strong>. We have also provided augmented versions of luz metrics for deep supervision: <strong>luz_metric_accuracyDS()</strong>, <strong>luz_metric_binary_accuracyDS()</strong>, and <strong>luz_metric_binary_accuracy_with_logitsDS()</strong>.</p>
</section>
<section id="selecting-a-learning-rate" class="level3">
<h3 class="anchored" data-anchor-id="selecting-a-learning-rate">Selecting a learning rate</h3>
<p>Generally, the learning rate is an important hyperparameter. A large learning rate may cause the optimization process to pass over the optimal parameters. In contrast, a low learning rate my cause the learning process to get “stuck” in a local minimum and/or increase the time required for the learning process to converge. One means to select a learning rate is described in <a href="https://arxiv.org/abs/1506.01186">this</a> paper by Leslie Smith. This learning rate finder process works by running a fast training iteration where the learning rate starts at a very low value and is incrementally increased to a very large value with the learning rate changing after each data batch. The optimal minimum learning rate is the one at which the loss decreased the fastest while the optimal maximum learning rate is ten times greater than the minimum. It has also been suggested that optimizers that use adaptive learning rates, such as Adam, can benefit from learning rate optimization.</p>
<p>The luz package provides an implementation of a learning rate finder via the <strong>luz_finder()</strong> function. Our examples will demonstrate the use of a this learning rate finder. Generally, we suggest using this function to select an appropriate learning rate or range of learning rates.</p>
<p>luz also provides the <strong>luz_callback_lr_scheduler()</strong> function for defining and implementing callbacks to change or adapt the learning rate during the training process. This function can use a variety of learning rate schedules provided by <strong>torch</strong> including the following (provided descriptions are from the torch documentation):</p>
<ul>
<li><strong>lr_lambda()</strong>: sets the learning rate of each parameter group to the initial learning times a given function.</li>
<li><strong>lr_multiplicative()</strong>: multiplies the learning rate of each parameter group by a factor given in the specified function.</li>
<li><strong>lr_one_cycle()</strong>: implement on cycle learning after <a href="https://arxiv.org/abs/1708.07120">Smith and Topin (2018)</a>.</li>
<li><strong>lr_reduce on plateau()</strong>: reduce learning rate when model stops improving.</li>
</ul>
<p>We have personally found one-cycle learning to be useful and recommend that it be experimented with.</p>
</section>
<section id="incorporating-luz-callbacks" class="level3">
<h3 class="anchored" data-anchor-id="incorporating-luz-callbacks">Incorporating luz callbacks</h3>
<p>The luz package provides additional callbacks that can be very useful during the learning process. Several of these will be demonstrated in our provided examples. For example, <strong>luz_callback_early_stopping()</strong> can be used to stop the learning process early if the model is no longer improving based on the loss or an assessment metric of interest, either calculated from the training or validation data. <strong>luz_callback_csv_logger()</strong> allow for logging calculated losses and metrics for the training and validation data to disk as a CSV file. <strong>luz_callback_model_checkpoint()</strong> can be used to save models to disk after each epoch or only if the model has improved based on the loss or an assessment metric. You can even define custom callbacks using the <strong>luz_callback()</strong> function.</p>
</section>
</section>
<section id="model-assessment-and-inference" class="level2">
<h2 class="anchored" data-anchor-id="model-assessment-and-inference">Model assessment and inference</h2>
<section id="assessment" class="level3">
<h3 class="anchored" data-anchor-id="assessment">Assessment</h3>
<p>Once a model has been trained, the results should be assessed using a <strong>testing</strong> dataset that is separate and non-overlapping with the training and validation sets. geodl provides several functions for assessing models. First, the <strong>viewBatchPreds()</strong> function allows for visualizing a batch of predictions, reference masks, and predictor variables that were created using <strong>defineSegDataSet()</strong> or <strong>defineSegDataSetDS()</strong> and a DataLoader and subsequently predicted with a trained model. The luz package provides additional routines, which will be demonstrated in our examples, for obtaining accuracy assessment metrics and losses for an entire testing set fed to the model as batches. Effectively, this process implements the same loss and accuracy assessment calculations used in the training loop for the training and validation data.</p>
<p>geodl provides the <strong>assessPnts()</strong> function that allows for performing assessments at point locations. This requires that reference and predicted classes be extracted at point locations and stored in a data frame. The <strong>assessRaster()</strong> function allows for assessment using all cells in an extent as opposed to point locations. This method can be used when you have reference labels and predictions for all pixels in an extent.</p>
<p>These functions will generate a set of summary metrics when provided reference and predicted classes. For a multiclass classification problem, a confusion matrix is produced with the columns representing the reference data and the rows representing the predictions. The following metrics are calculated: overall accuracy (OA), 95% confidence interval for OA (OAU and OAL), the Kappa statistic, map image classification efficacy (MICE), average class user’s accuracy (aUA), average class producer’s accuracy (aPA), average class F1-score, overall error (Error), allocation disagreement (Allocation), quantity disagreement (Quantity), exchange disagreement (Exchange), and shift disagreement (shift). For average class user’s accuracy, producer’s accuracy, and F1-score, macro-averaging is used where all classes are equally weighted. As mentioned above, it is not necessary to return micro-averaged producer’s accuracy, user’s accuracy, and F1-score since these are equivalent to overall accuracy. For a multiclass classification, all class user’s and producer’s accuracies are also returned. For a binary classification problem, a confusion matrix is returned along with the following metrics: overall accuracy (OA), overall accuracy 95% confidence interval (OAU and OAL), the Kappa statistic (Kappa), map image classification efficacy (MICE), precision (Precision), recall (Recall), F1-score (F1), negative predictive value (NPV), specificity (Specificity), overall error (Error), allocation disagreement (Allocation), quantity disagreement (Quantity), exchange disagreement (Exchange), and shift disagreement (shift). Results are returned as a list object. This function makes use of the <a href="https://topepo.github.io/caret/">caret</a>, <a href="https://cran.r-project.org/web/packages/diffeR/index.html">diffeR</a>, and <a href="https://cran.r-project.org/web/packages/rfUtilities/index.html">rfUtilities</a> packages.</p>
<p>For assessing map output, we generally recommend using a testing set that honors the true landscape proportions of each class. When a confusion matrix is generated using proportions that approximate the true landscape proportions, it is termed a <strong>population matrix</strong> or <strong>population confusion matrix</strong>.</p>
</section>
<section id="inference" class="level3">
<h3 class="anchored" data-anchor-id="inference">Inference</h3>
<p>A trained model can be used to infer to new raster data as long as the input variables are the same as those used to train the model. The <strong>predictSpatial()</strong> function allows for predicting to a raster extent. In order to process large raster extents, chips are extracted from the larger extent relative to the <em>chpSize</em> parameter. Overlap between chips are specified using the <em>stride_x</em> and <em>stride_y</em> parameters. We generally recommend using an overlap of at least 25% between adjacent chips. It has generally be found that predictions nearer to the margin of a chip are poorer or less accurate, likely due to the lack of a full set of neighboring pixels. As a result, the <em>crop</em> parameter can be set to remove outer rows and columns of pixels and not include them in the final, merged product. Using a combination of overlap with the <em>stride_x</em> and <em>stride_y</em> parameters in combination with cropping (<em>crop</em>) allow for only predictions in the center of each processed chip to be included in the final, merged product.</p>
<p>For a binary classification and when the <em>probs</em> argument is set to FALSE, a single-band raster in which each cell is labeled as 1 (predicted to the positive case) or 0 (background or predicted to the negative case) is returned. If the <em>probs</em> argument is set to TRUE, the positive case probability will be returned by passing the predicted logit at each cell through a sigmoid activation. For a multiclass classification and when the <em>probs</em> argument is set to FALSE, a single-band raster will be returned in which each cell holds the index or numeric code for the predicted class. If the <em>probs</em> argument is set to TRUE, a multiband raster will be produced in which each band holds the predicted probability for each class at each pixel location generated by passing the logits through a softmax activation. All class probabilities at a cell will sum to 1, minus any rounding error.</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>